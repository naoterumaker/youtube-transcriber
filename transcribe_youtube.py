#!/usr/bin/env python3
import re
import os
import csv
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import time
from pathlib import Path

import click
import pandas as pd
from tqdm import tqdm
from youtube_transcript_api import YouTubeTranscriptApi
from googleapiclient.discovery import build
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Global variables for API key rotation
_current_api_key_index = 0
_api_keys = []


def parse_duration(duration: str) -> int:
    """ISO 8601 duration (PT1H2M3S) ã‚’ç§’æ•°ã«å¤‰æ›"""
    if not duration:
        return 0
    
    # PT1H2M3S ã®ã‚ˆã†ãªå½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
    pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
    match = re.match(pattern, duration)
    
    if not match:
        return 0
    
    hours = int(match.group(1) or 0)
    minutes = int(match.group(2) or 0)
    seconds = int(match.group(3) or 0)
    
    return hours * 3600 + minutes * 60 + seconds


def format_duration(seconds: int) -> str:
    """ç§’æ•°ã‚’ HH:MM:SS å½¢å¼ã«å¤‰æ›"""
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    secs = seconds % 60
    
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    else:
        return f"{minutes:02d}:{secs:02d}"


def calculate_engagement_metrics(video_stats: Dict, subscriber_count: int) -> Dict[str, float]:
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—"""
    view_count = int(video_stats.get('viewCount', 0))
    like_count = int(video_stats.get('likeCount', 0))
    comment_count = int(video_stats.get('commentCount', 0))
    
    # æ‹¡æ•£ç‡ (è¦–è´å›æ•° / ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…æ•°)
    spread_rate = (view_count / subscriber_count * 100) if subscriber_count > 0 else 0
    
    # è¦–è´ã‚³ãƒ¡ãƒ³ãƒˆç‡ (ã‚³ãƒ¡ãƒ³ãƒˆæ•° / è¦–è´å›æ•° * 100)
    comment_rate = (comment_count / view_count * 100) if view_count > 0 else 0
    
    # è¦–è´é«˜è©•ä¾¡ç‡ (é«˜è©•ä¾¡æ•° / è¦–è´å›æ•° * 100)
    like_rate = (like_count / view_count * 100) if view_count > 0 else 0
    
    # è¦–è´ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ ((é«˜è©•ä¾¡æ•° + ã‚³ãƒ¡ãƒ³ãƒˆæ•°) / è¦–è´å›æ•° * 100)
    engagement_rate = ((like_count + comment_count) / view_count * 100) if view_count > 0 else 0
    
    return {
        'spread_rate': round(spread_rate, 4),
        'comment_rate': round(comment_rate, 4),
        'like_rate': round(like_rate, 4),
        'engagement_rate': round(engagement_rate, 4)
    }


def get_date_range_from_period(period: str) -> tuple[Optional[datetime], Optional[datetime]]:
    """æœŸé–“æ–‡å­—åˆ—ã‹ã‚‰æ—¥ä»˜ç¯„å›²ã‚’å–å¾—"""
    now = datetime.now()
    
    if period == "3months":
        start_date = now - timedelta(days=90)
        return start_date, now
    elif period == "6months":
        start_date = now - timedelta(days=180)
        return start_date, now
    elif period == "1year":
        start_date = now - timedelta(days=365)
        return start_date, now
    elif period == "all":
        return None, None  # å…¨æœŸé–“
    else:
        return None, None


def get_recommended_video_count(period: str, total_videos: int) -> Optional[int]:
    """æœŸé–“ã«å¿œã˜ãŸæ¨å¥¨å‹•ç”»æ•°ã‚’å–å¾—"""
    if period == "3months":
        # 3ã‹æœˆãªã‚‰æœ€å¤§100æœ¬ç¨‹åº¦ãŒé©åˆ‡
        return min(100, total_videos)
    elif period == "6months":
        # åŠå¹´ãªã‚‰æœ€å¤§200æœ¬ç¨‹åº¦
        return min(200, total_videos)
    elif period == "1year":
        # 1å¹´ãªã‚‰æœ€å¤§500æœ¬ç¨‹åº¦
        return min(500, total_videos)
    elif period == "all":
        # å…¨æœŸé–“ã®å ´åˆã¯åˆ¶é™ãªã—ï¼ˆãŸã ã—å®Ÿç”¨çš„ãªä¸Šé™ã‚’è¨­å®šï¼‰
        return min(1000, total_videos)
    else:
        return None


def prompt_period_selection() -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æœŸé–“é¸æŠã‚’ä¿ƒã™"""
    click.echo("\nğŸ“… å–å¾—æœŸé–“ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    click.echo("1ï¸âƒ£  ç›´è¿‘3ã‹æœˆ")
    click.echo("2ï¸âƒ£  ç›´è¿‘åŠå¹´")
    click.echo("3ï¸âƒ£  ç›´è¿‘1å¹´")
    click.echo("4ï¸âƒ£  å…¨æœŸé–“")
    
    while True:
        choice = click.prompt("\né¸æŠã—ã¦ãã ã•ã„ (1-4)", type=str).strip()
        
        if choice == "1":
            return "3months"
        elif choice == "2":
            return "6months"
        elif choice == "3":
            return "1year"
        elif choice == "4":
            return "all"
        else:
            click.echo("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚1-4ã®æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")


def create_output_directory(base_dir: str, channel_name: str) -> Path:
    """ãƒãƒ£ãƒ³ãƒãƒ«ç”¨ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    # å®‰å…¨ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½œæˆ
    safe_channel_name = re.sub(r'[<>:"/\\|?*]', '_', channel_name)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_path = Path(base_dir) / f"{safe_channel_name}_{timestamp}"
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    (output_path / "transcripts").mkdir(exist_ok=True)
    (output_path / "data").mkdir(exist_ok=True)
    
    return output_path


def extract_video_id(url_or_id: str) -> Optional[str]:
    patterns = [
        r"(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/embed/|youtube\.com/v/)([^#&?]{11})",
        r"^([a-zA-Z0-9_-]{11})$",
    ]
    for pattern in patterns:
        m = re.search(pattern, url_or_id)
        if m:
            return m.group(1)
    return None


def load_api_keys():
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¤‡æ•°ã®APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
    global _api_keys
    if _api_keys:  # æ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        return _api_keys
    
    # è¤‡æ•°ã®APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    keys = []
    for i in range(1, 6):  # KEY_1 ã‹ã‚‰ KEY_5 ã¾ã§
        key = os.getenv(f'YOUTUBE_API_KEY_{i}')
        if key and key.strip():
            keys.append(key.strip())
    
    # è¤‡æ•°ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å˜ä¸€ã‚­ãƒ¼ã‚’ç¢ºèª
    if not keys:
        single_key = os.getenv('YOUTUBE_API_KEY')
        if single_key and single_key.strip():
            keys.append(single_key.strip())
    
    if not keys:
        raise ValueError(
            "No YouTube API keys found. Please set YOUTUBE_API_KEY_1, YOUTUBE_API_KEY_2, etc. "
            "or YOUTUBE_API_KEY in your .env file."
        )
    
    _api_keys = keys
    click.echo(f"Loaded {len(_api_keys)} API key(s)")
    return _api_keys


def get_current_api_key():
    """ç¾åœ¨ã®APIã‚­ãƒ¼ã‚’å–å¾—ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    global _current_api_key_index
    keys = load_api_keys()
    
    if _current_api_key_index >= len(keys):
        _current_api_key_index = 0
    
    return keys[_current_api_key_index]


def rotate_api_key():
    """æ¬¡ã®APIã‚­ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆ"""
    global _current_api_key_index
    keys = load_api_keys()
    _current_api_key_index = (_current_api_key_index + 1) % len(keys)
    click.echo(f"Switched to API key {_current_api_key_index + 1}/{len(keys)}")


def get_youtube_service():
    """YouTube Data API v3 service ã‚’å–å¾—"""
    api_key = get_current_api_key()
    return build('youtube', 'v3', developerKey=api_key)


def handle_api_error(error, operation_name: str):
    """APIã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚­ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
    error_str = str(error).lower()
    
    if 'quota' in error_str or '403' in error_str or 'exceeded' in error_str:
        click.echo(f"API quota exceeded during {operation_name}. Trying next API key...")
        rotate_api_key()
        return True  # ãƒªãƒˆãƒ©ã‚¤å¯èƒ½
    else:
        click.echo(f"Error during {operation_name}: {error}", err=True)
        return False  # ãƒªãƒˆãƒ©ã‚¤ä¸å¯èƒ½


def get_channel_id_from_name(channel_name: str) -> Optional[str]:
    """ãƒãƒ£ãƒ³ãƒãƒ«åã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—"""
    max_retries = len(load_api_keys())
    
    for attempt in range(max_retries):
        try:
            youtube = get_youtube_service()
            
            # ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢
            search_response = youtube.search().list(
                q=channel_name,
                type='channel',
                part='id',
                maxResults=1
            ).execute()
            
            if search_response['items']:
                return search_response['items'][0]['id']['channelId']
            return None
            
        except Exception as e:
            if handle_api_error(e, "channel search"):
                continue  # æ¬¡ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤
            else:
                return None
    
    click.echo("All API keys exhausted for channel search", err=True)
    return None


def get_channel_videos(channel_id: str, max_results: Optional[int] = None, 
                      start_date: Optional[datetime] = None, end_date: Optional[datetime] = None) -> List[str]:
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»IDã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    video_ids = []
    next_page_token = None
    max_retries = len(load_api_keys())
    
    while True:
        success = False
        
        for attempt in range(max_retries):
            try:
                youtube = get_youtube_service()
                
                # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                search_params = {
                    'channelId': channel_id,
                    'type': 'video',
                    'part': 'id',
                    'maxResults': 50,  # APIåˆ¶é™å†…ã§ã®æœ€å¤§å€¤
                    'order': 'date'
                }
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ 
                if start_date:
                    search_params['publishedAfter'] = start_date.isoformat() + 'Z'
                if end_date:
                    search_params['publishedBefore'] = end_date.isoformat() + 'Z'
                if next_page_token:
                    search_params['pageToken'] = next_page_token
                
                # ãƒãƒ£ãƒ³ãƒãƒ«ã®å‹•ç”»ã‚’æ¤œç´¢
                search_response = youtube.search().list(**search_params).execute()
                
                # å‹•ç”»IDã‚’åé›†
                for item in search_response['items']:
                    video_ids.append(item['id']['videoId'])
                    if max_results and len(video_ids) >= max_results:
                        return video_ids[:max_results]
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                next_page_token = search_response.get('nextPageToken')
                success = True
                break
                
            except Exception as e:
                if handle_api_error(e, "video list fetch"):
                    continue  # æ¬¡ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤
                else:
                    return video_ids  # ã‚¨ãƒ©ãƒ¼ã§çµ‚äº†ã€ã“ã‚Œã¾ã§ã®çµæœã‚’è¿”ã™
        
        if not success:
            click.echo("All API keys exhausted for video list fetch", err=True)
            break
            
        if not next_page_token:
            break
            
        # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
        time.sleep(0.1)
    
    return video_ids


def get_video_info(video_id: str) -> Dict[str, Any]:
    """å‹•ç”»ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    max_retries = len(load_api_keys())
    
    for attempt in range(max_retries):
        try:
            youtube = get_youtube_service()
            
            video_response = youtube.videos().list(
                part='snippet,statistics,contentDetails',
                id=video_id
            ).execute()
            
            if video_response['items']:
                return video_response['items'][0]
            return {}
            
        except Exception as e:
            if handle_api_error(e, f"video info fetch for {video_id}"):
                continue  # æ¬¡ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤
            else:
                return {}
    
    click.echo(f"All API keys exhausted for video info fetch: {video_id}", err=True)
    return {}


def fetch_transcript(video_id: str) -> str:
    api = YouTubeTranscriptApi()
    preferred_languages = ["ja", "ja-JP", "en", "en-US"]
    last_error: Optional[Exception] = None
    for lang in preferred_languages:
        try:
            chunks = api.fetch(video_id, languages=[lang])
            return " ".join([c.text for c in chunks])
        except Exception as e:  # noqa: BLE001
            last_error = e
            continue
    try:
        chunks = api.fetch(video_id)
        return " ".join([c.text for c in chunks])
    except Exception as e:  # noqa: BLE001
        raise RuntimeError(f"Could not fetch transcript: {e or last_error}")


def ensure_parent_dir(path: str) -> None:
    parent = os.path.dirname(path)
    if parent and not os.path.exists(parent):
        os.makedirs(parent, exist_ok=True)


def fetch_channel_transcripts(channel_name: str, output_dir: str, max_videos: Optional[int] = None, 
                             fmt: str = "md", include_csv: bool = True, period: Optional[str] = None) -> None:
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã®æ–‡å­—èµ·ã“ã—ã¨CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    click.echo(f"ğŸ” Searching for channel: {channel_name}")
    
    # ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’å–å¾—
    channel_id = get_channel_id_from_name(channel_name)
    if not channel_id:
        raise click.ClickException(f"Channel not found: {channel_name}")
    
    click.echo(f"âœ… Found channel ID: {channel_id}")
    
    # ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã‚’å–å¾—
    channel_info = get_channel_info(channel_id)
    if not channel_info:
        raise click.ClickException(f"Failed to get channel information")
    
    channel_title = channel_info['snippet']['title']
    subscriber_count = int(channel_info['statistics'].get('subscriberCount', 0))
    
    click.echo(f"ğŸ“º Channel: {channel_title}")
    click.echo(f"ğŸ‘¥ Subscribers: {subscriber_count:,}")
    
    # æœŸé–“é¸æŠï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼‰
    if period is None:
        period = prompt_period_selection()
    
    # æ—¥ä»˜ç¯„å›²ã‚’å–å¾—
    start_date, end_date = get_date_range_from_period(period)
    
    # æœŸé–“æƒ…å ±ã‚’è¡¨ç¤º
    if period == "3months":
        click.echo("ğŸ“… å–å¾—æœŸé–“: ç›´è¿‘3ã‹æœˆ")
    elif period == "6months":
        click.echo("ğŸ“… å–å¾—æœŸé–“: ç›´è¿‘åŠå¹´")
    elif period == "1year":
        click.echo("ğŸ“… å–å¾—æœŸé–“: ç›´è¿‘1å¹´")
    elif period == "all":
        click.echo("ğŸ“… å–å¾—æœŸé–“: å…¨æœŸé–“")
    
    # å‹•ç”»IDãƒªã‚¹ãƒˆã‚’å–å¾—
    click.echo("ğŸ“‹ Fetching video list...")
    video_ids = get_channel_videos(channel_id, max_videos, start_date, end_date)
    
    if not video_ids:
        click.echo("âŒ No videos found in this channel.")
        return
    
    click.echo(f"ğŸ“¹ Found {len(video_ids)} videos")
    
    # æ¨å¥¨å‹•ç”»æ•°ã®è¡¨ç¤ºã¨é©ç”¨
    if max_videos is None:
        recommended_count = get_recommended_video_count(period, len(video_ids))
        if recommended_count and recommended_count < len(video_ids):
            click.echo(f"ğŸ’¡ æ¨å¥¨å‡¦ç†æ•°: {recommended_count}æœ¬ (æœŸé–“: {period})")
            if click.confirm(f"æ¨å¥¨æ•° {recommended_count}æœ¬ã§å‡¦ç†ã—ã¾ã™ã‹ï¼Ÿ", default=True):
                video_ids = video_ids[:recommended_count]
                click.echo(f"ğŸ“¹ Processing {len(video_ids)} videos (recommended limit applied)")
            else:
                click.echo(f"ğŸ“¹ Processing all {len(video_ids)} videos")
        else:
            click.echo(f"ğŸ“¹ Processing all {len(video_ids)} videos")
    else:
        video_ids = video_ids[:max_videos]
        click.echo(f"ğŸ“¹ Processing {len(video_ids)} videos (user limit applied)")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_path = create_output_directory(output_dir, channel_name)
    click.echo(f"ğŸ“ Output directory: {output_path}")
    
    # CSVãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒªã‚¹ãƒˆ
    csv_data = []
    csv_headers = [
        'ãƒã‚§ãƒƒã‚¯', 'ã‚¿ã‚¤ãƒˆãƒ«', 'å‹•ç”»ãƒªãƒ³ã‚¯', 'ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒ', 'ãƒãƒ£ãƒ³ãƒãƒ«å', 
        'æŠ•ç¨¿æ—¥', 'è¦–è´å›æ•°', 'é«˜è©•ä¾¡æ•°', 'ã‚³ãƒ¡ãƒ³ãƒˆæ•°', 'å‹•ç”»æ™‚é–“', 
        'ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²è€…æ•°', 'æ‹¡æ•£ç‡', 'è¦–è´ã‚³ãƒ¡ãƒ³ãƒˆç‡', 'è¦–è´é«˜è©•ä¾¡ç‡', 'è¦–è´ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡'
    ]
    
    successful_transcripts = 0
    failed_transcripts = 0
    
    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã§å„å‹•ç”»ã‚’å‡¦ç†
    with tqdm(total=len(video_ids), desc="Processing videos") as pbar:
        for i, video_id in enumerate(video_ids, 1):
            try:
                pbar.set_description(f"Processing video {i}/{len(video_ids)}")
                
                # å‹•ç”»æƒ…å ±ã‚’å–å¾—
                video_info = get_video_info(video_id)
                if not video_info or 'snippet' not in video_info:
                    pbar.write(f"âš ï¸  Skipping {video_id}: No video info available")
                    failed_transcripts += 1
                    pbar.update(1)
                    continue
                
                snippet = video_info['snippet']
                statistics = video_info.get('statistics', {})
                content_details = video_info.get('contentDetails', {})
                
                video_title = snippet.get('title', 'Unknown')
                published_at = snippet.get('publishedAt', '')
                
                # çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                view_count = int(statistics.get('viewCount', 0))
                like_count = int(statistics.get('likeCount', 0))
                comment_count = int(statistics.get('commentCount', 0))
                
                # å‹•ç”»æ™‚é–“ã‚’å–å¾—ãƒ»å¤‰æ›
                duration_iso = content_details.get('duration', 'PT0S')
                duration_seconds = parse_duration(duration_iso)
                duration_formatted = format_duration(duration_seconds)
                
                # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã‚’è¨ˆç®—
                metrics = calculate_engagement_metrics(statistics, subscriber_count)
                
                # CSVãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                csv_row = [
                    '',  # ãƒã‚§ãƒƒã‚¯ï¼ˆç©ºæ¬„ï¼‰
                    video_title,
                    f"https://www.youtube.com/watch?v={video_id}",
                    f"https://i.ytimg.com/vi/{video_id}/maxresdefault.jpg",
                    channel_title,
                    datetime.fromisoformat(published_at.replace('Z', '+00:00')).strftime('%Y/%m/%d') if published_at else '',
                    view_count,
                    like_count,
                    comment_count,
                    duration_formatted,
                    subscriber_count,
                    metrics['spread_rate'],
                    metrics['comment_rate'],
                    metrics['like_rate'],
                    metrics['engagement_rate']
                ]
                csv_data.append(csv_row)
                
                # æ–‡å­—èµ·ã“ã—ã‚’å–å¾—
                try:
                    transcript_text = fetch_transcript(video_id)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆå®‰å…¨ãªæ–‡å­—ã®ã¿ä½¿ç”¨ï¼‰
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', video_title)[:50]
                    extension = "md" if fmt == "md" else "txt"
                    filename = f"{safe_title}_{video_id}.{extension}"
                    transcript_path = output_path / "transcripts" / filename
                    
                    # å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    url = f"https://www.youtube.com/watch?v={video_id}"
                    formatted_content = format_output(transcript_text, url, fmt, video_title)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                    with open(transcript_path, "w", encoding="utf-8") as f:
                        f.write(formatted_content)
                    
                    successful_transcripts += 1
                    pbar.write(f"âœ… Saved transcript: {filename}")
                    
                except Exception as transcript_error:
                    pbar.write(f"âš ï¸  Failed to get transcript for {video_id}: {transcript_error}")
                    # CSVãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒï¼ˆæ–‡å­—èµ·ã“ã—ãŒå¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ã¯æœ‰åŠ¹ï¼‰
                
                # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
                time.sleep(0.3)
                
            except Exception as e:
                pbar.write(f"âŒ Failed to process {video_id}: {e}")
                failed_transcripts += 1
            
            pbar.update(1)
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    if include_csv and csv_data:
        csv_path = output_path / "data" / f"{channel_title}_analysis.csv"
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(csv_headers)
            writer.writerows(csv_data)
        
        click.echo(f"ğŸ“Š CSV saved: {csv_path}")
        
        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç”Ÿæˆ
        try:
            excel_path = output_path / "data" / f"{channel_title}_analysis.xlsx"
            df = pd.DataFrame(csv_data, columns=csv_headers)
            df.to_excel(excel_path, index=False, engine='openpyxl')
            click.echo(f"ğŸ“ˆ Excel saved: {excel_path}")
        except ImportError:
            click.echo("âš ï¸  openpyxl not installed. Excel file not generated.")
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    generate_summary_report(output_path, channel_title, len(video_ids), successful_transcripts, failed_transcripts, csv_data)
    
    click.echo(f"\nğŸ‰ Completed!")
    click.echo(f"ğŸ“Š Total videos: {len(video_ids)}")
    click.echo(f"âœ… Successful transcripts: {successful_transcripts}")
    click.echo(f"âŒ Failed transcripts: {failed_transcripts}")
    click.echo(f"ğŸ“ Output directory: {output_path}")


def get_channel_info(channel_id: str) -> Optional[Dict[str, Any]]:
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    max_retries = len(load_api_keys())
    
    for attempt in range(max_retries):
        try:
            youtube = get_youtube_service()
            
            channel_response = youtube.channels().list(
                part='snippet,statistics',
                id=channel_id
            ).execute()
            
            if channel_response['items']:
                return channel_response['items'][0]
            return None
            
        except Exception as e:
            if handle_api_error(e, f"channel info fetch for {channel_id}"):
                continue
            else:
                return None
    
    click.echo(f"All API keys exhausted for channel info fetch: {channel_id}", err=True)
    return None


def generate_summary_report(output_path: Path, channel_name: str, total_videos: int, 
                          successful: int, failed: int, csv_data: List) -> None:
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    report_path = output_path / "summary_report.md"
    
    # åŸºæœ¬çµ±è¨ˆã‚’è¨ˆç®—
    if csv_data:
        df = pd.DataFrame(csv_data[1:], columns=csv_data[0] if csv_data else [])  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
        
        try:
            # æ•°å€¤åˆ—ã‚’å¤‰æ›
            numeric_cols = ['è¦–è´å›æ•°', 'é«˜è©•ä¾¡æ•°', 'ã‚³ãƒ¡ãƒ³ãƒˆæ•°', 'æ‹¡æ•£ç‡', 'è¦–è´ã‚³ãƒ¡ãƒ³ãƒˆç‡', 'è¦–è´é«˜è©•ä¾¡ç‡', 'è¦–è´ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            avg_views = df['è¦–è´å›æ•°'].mean() if 'è¦–è´å›æ•°' in df.columns else 0
            avg_likes = df['é«˜è©•ä¾¡æ•°'].mean() if 'é«˜è©•ä¾¡æ•°' in df.columns else 0
            avg_comments = df['ã‚³ãƒ¡ãƒ³ãƒˆæ•°'].mean() if 'ã‚³ãƒ¡ãƒ³ãƒˆæ•°' in df.columns else 0
            avg_engagement = df['è¦–è´ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡'].mean() if 'è¦–è´ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡' in df.columns else 0
            
        except Exception:
            avg_views = avg_likes = avg_comments = avg_engagement = 0
    else:
        avg_views = avg_likes = avg_comments = avg_engagement = 0
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report_content = f"""# YouTube Channel Analysis Report

## ğŸ“Š Channel Information
- **Channel Name:** {channel_name}
- **Analysis Date:** {timestamp}
- **Total Videos Found:** {total_videos}

## ğŸ¯ Processing Results
- âœ… **Successful Transcripts:** {successful}
- âŒ **Failed Transcripts:** {failed}
- ğŸ“ˆ **Success Rate:** {(successful/total_videos*100):.1f}%

## ğŸ“ˆ Channel Statistics
- ğŸ‘€ **Average Views:** {avg_views:,.0f}
- ğŸ‘ **Average Likes:** {avg_likes:,.0f}
- ğŸ’¬ **Average Comments:** {avg_comments:,.0f}
- ğŸ”¥ **Average Engagement Rate:** {avg_engagement:.2f}%

## ğŸ“ Output Structure
```
{output_path.name}/
â”œâ”€â”€ transcripts/          # æ–‡å­—èµ·ã“ã—ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ data/                 # CSVãƒ»Excelãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ {channel_name}_analysis.csv
â”‚   â””â”€â”€ {channel_name}_analysis.xlsx
â””â”€â”€ summary_report.md     # ã“ã®ãƒ¬ãƒãƒ¼ãƒˆ
```

## ğŸ”§ Generated Files
- **Transcript Files:** {successful} files in `transcripts/` directory
- **CSV Data:** `data/{channel_name}_analysis.csv`
- **Excel Data:** `data/{channel_name}_analysis.xlsx`
- **Summary Report:** `summary_report.md`

---
Generated by YouTube Transcriber
"""
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    click.echo(f"ğŸ“‹ Summary report saved: {report_path}")


def format_output(text: str, url: str, fmt: str, title: str = None) -> str:
    if fmt == "txt":
        return text
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    title_section = ""
    if title:
        title_section = f"**Title:** {title}\n\n"
    
    return (
        f"# YouTube Transcript\n\n"
        f"{title_section}"
        f"**URL:** {url}\n\n"
        f"**Generated:** {ts}\n\n"
        f"---\n\n{text}\n"
    )


@click.group()
def cli():
    """YouTube Transcriber - å‹•ç”»ã‚„ãƒãƒ£ãƒ³ãƒãƒ«ã®æ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ«"""
    pass


@cli.command()
@click.argument("url")
@click.option("--output", "output_path", default="output/transcript.md", help="Output file path")
@click.option("--format", "fmt", type=click.Choice(["md", "txt"]), default="md", help="Output format")
def video(url: str, output_path: str, fmt: str) -> None:
    """å˜ä¸€ã®å‹•ç”»ã‚’æ–‡å­—èµ·ã“ã—"""
    video_id = extract_video_id(url)
    if not video_id:
        raise click.ClickException("Invalid YouTube URL or ID")

    text = fetch_transcript(video_id)
    ensure_parent_dir(output_path)
    body = format_output(text, url, fmt)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(body)
    click.echo(f"Saved transcript to {output_path}")


@cli.command()
@click.argument("channel_name")
@click.option("--output-dir", default="output/channel_analysis", help="Output directory for channel analysis")
@click.option("--format", "fmt", type=click.Choice(["md", "txt"]), default="md", help="Output format for transcripts")
@click.option("--max-videos", type=int, help="Maximum number of videos to process")
@click.option("--period", type=click.Choice(["3months", "6months", "1year", "all"]), help="Time period to fetch videos from")
@click.option("--no-csv", is_flag=True, help="Skip CSV/Excel generation")
@click.option("--transcripts-only", is_flag=True, help="Generate transcripts only (no analysis data)")
def channel(channel_name: str, output_dir: str, fmt: str, max_videos: Optional[int], 
           period: Optional[str], no_csv: bool, transcripts_only: bool) -> None:
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®å…¨å‹•ç”»ã‚’æ–‡å­—èµ·ã“ã—ï¼‹åˆ†æãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    try:
        include_csv = not no_csv and not transcripts_only
        fetch_channel_transcripts(channel_name, output_dir, max_videos, fmt, include_csv, period)
    except Exception as e:
        raise click.ClickException(str(e))


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«ã€å¼•æ•°ãªã—ã§å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã¯å˜ä¸€å‹•ç”»ãƒ¢ãƒ¼ãƒ‰ã¨ã—ã¦å‹•ä½œ
@click.command()
@click.argument("url")
@click.option("--output", "output_path", default="output/transcript.md", help="Output file path")
@click.option("--format", "fmt", type=click.Choice(["md", "txt"]), default="md", help="Output format")
def main(url: str, output_path: str, fmt: str) -> None:
    """å˜ä¸€ã®å‹•ç”»ã‚’æ–‡å­—èµ·ã“ã—ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
    video_id = extract_video_id(url)
    if not video_id:
        raise click.ClickException("Invalid YouTube URL or ID")

    text = fetch_transcript(video_id)
    ensure_parent_dir(output_path)
    body = format_output(text, url, fmt)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(body)
    click.echo(f"Saved transcript to {output_path}")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] in ['video', 'channel']:
        cli()
    else:
        main()


